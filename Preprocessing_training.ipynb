{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlPGjDn1rmftBOJQ8LBcCD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RUND0WN75/CSCI158-Applied-Biometric-Security-FRESNO-STATE/blob/main/Preprocessing_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Libraries for preprocessing."
      ],
      "metadata": {
        "id": "P6rAW83YWuXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Dowload Dataset\n",
        "#create data loader\n",
        "# build model\n",
        "# train\n",
        "# save trained model\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001"
      ],
      "metadata": {
        "id": "HGwuTfZbWp0a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the classes for forward feeding and downloading datasets"
      ],
      "metadata": {
        "id": "LGW_5RtMW178"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardnet(nn.Module):\n",
        "    def __init__(self): #constructor\n",
        "        super().__init__()\n",
        "        self.flatten= nn.Flatten() #john is just an arbitrary name\n",
        "        self.dense_layers = nn.Sequential( #allows to pack together multiple layers for sequentially\n",
        "            nn.Linear(28*28, 256), #input and output features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10), #10 is the number of data sets\n",
        "        )\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    #specifies the data flow\n",
        "    def forward(self, input_data): #how to manipulated the data\n",
        "        flattened_data = self.flatten(input_data)\n",
        "        logits = self.dense_layers(flattened_data) #outputs\n",
        "        predictions = self.softmax(logits)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "def download_dataset():\n",
        "    train_data = datasets.MNIST(\n",
        "        root= \"data\",\n",
        "        download= True,\n",
        "        train= True,\n",
        "        transform= ToTensor()     #takes image in ans shapes tensor\n",
        "    )\n",
        "\n",
        "    validation_data = datasets.MNIST(\n",
        "        root= \"data\",\n",
        "        download= True,\n",
        "        train= True,\n",
        "        transform= ToTensor()     #takes image in ans shapes tensor\n",
        "    )\n",
        "\n",
        "    return train_data, validation_data\n",
        "\n",
        "def train_one_epoch(model, data_loader, loss_fn, optimiser, device):\n",
        "    for inputs, targets in data_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "\n",
        "        #calculate loss\n",
        "        predictions = model(inputs)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "\n",
        "        #back propogate the loss and update the weights\n",
        "        optimiser.zero_grad() #resets gradients to zero\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
        "    for i in range(epochs):\n",
        "        print(f\"Epochs{i+1}\")\n",
        "        train_one_epoch(model, data_loader, loss_fn, optimiser, device)\n",
        "        print(\"-----------------\")\n",
        "    print(\"Training is done.\")"
      ],
      "metadata": {
        "id": "C858lGKCY7xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main function and how it creates, builds and instantiates the model. While also calculating for loss of entropy."
      ],
      "metadata": {
        "id": "5iPbw84oZAFq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs1w3OnGU36K",
        "outputId": "2edb1c0d-6f7d-43eb-c598-af8d51221d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 112879626.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 46970024.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28820657.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10861190.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "MNIIST dataset downloaded\n",
            "Using cpu device\n",
            "Epochs1\n",
            "Loss: 1.513471007347107\n",
            "-----------------\n",
            "Epochs2\n",
            "Loss: 1.4988842010498047\n",
            "-----------------\n",
            "Epochs3\n",
            "Loss: 1.4956494569778442\n",
            "-----------------\n",
            "Epochs4\n",
            "Loss: 1.48817777633667\n",
            "-----------------\n",
            "Epochs5\n",
            "Loss: 1.4794567823410034\n",
            "-----------------\n",
            "Epochs6\n",
            "Loss: 1.4746742248535156\n",
            "-----------------\n",
            "Epochs7\n",
            "Loss: 1.472772240638733\n",
            "-----------------\n",
            "Epochs8\n",
            "Loss: 1.4730957746505737\n",
            "-----------------\n",
            "Epochs9\n",
            "Loss: 1.4742625951766968\n",
            "-----------------\n",
            "Epochs10\n",
            "Loss: 1.4752616882324219\n",
            "-----------------\n",
            "Training is done.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_data, _ = download_dataset()\n",
        "    print(\"MNIIST dataset downloaded\")\n",
        "\n",
        "    #creating a data loader for the train set\n",
        "    train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "    #build a model\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    print(f\"Using {device} device\")\n",
        "\n",
        "    feed_foward_net = FeedForwardnet().to(device) #device is a string called CUDA (GPU)\n",
        "\n",
        "    #Instantiate loss and opt\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimiser = torch.optim.Adam(feed_foward_net.parameters(),lr= LEARNING_RATE)\n",
        "    #train the model\n",
        "    train(feed_foward_net, train_data_loader, loss_fn, optimiser, device, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "joWSEjHlWn-H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}